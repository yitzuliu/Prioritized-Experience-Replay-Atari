# Prioritized Experience Replay (PER) for Atari Ice Hockey

Training a Deep Q-Network (DQN) to play Atari Ice Hockey using Prioritized Experience Replay (PER) technique.

ä½¿ç”¨å„ªå…ˆç¶“é©—å›æ”¾ (PER) æŠ€è¡“è¨“ç·´æ·±åº¦ Q ç¶²çµ¡ (DQN) å­¸ç¿’ç© Atari å†°çƒéŠæˆ²ã€‚

![Atari Ice Hockey](https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/ALE-IceHockey-v5/poster.jpg)

## ğŸ“ Project Overview (å°ˆæ¡ˆæ¦‚è¿°)

This project implements a Deep Q-Network (DQN) with Prioritized Experience Replay (PER) to play the Atari Ice Hockey game. PER is an enhanced experience replay mechanism that prioritizes sampling of high-value experiences based on their importance (measured by TD-error). This approach significantly improves DQN's learning efficiency and performance.

æœ¬å°ˆæ¡ˆå¯¦ç¾äº†ä¸€å€‹å¸¶æœ‰å„ªå…ˆç¶“é©—å›æ”¾ (PER) çš„æ·±åº¦ Q ç¶²çµ¡ (DQN) ä¾†ç© Atari å†°çƒéŠæˆ²ã€‚å„ªå…ˆç¶“é©—å›æ”¾æ˜¯ä¸€ç¨®æ”¹é€²å‹ç¶“é©—å›æ”¾æ©Ÿåˆ¶ï¼Œå®ƒæ ¹æ“šæ¨£æœ¬çš„é‡è¦æ€§ï¼ˆç”± TD èª¤å·®æ¸¬é‡ï¼‰ä¾†å„ªå…ˆæ¡æ¨£é«˜åƒ¹å€¼çš„ç¶“é©—ã€‚é€™ç¨®æ–¹æ³•å¯ä»¥é¡¯è‘—æé«˜ DQN çš„å­¸ç¿’æ•ˆç‡å’Œæ€§èƒ½ã€‚

### ğŸ’¡ Key Features (ä¸»è¦ç‰¹é»)

- **Complete PER Implementation**: Using SumTree data structure for efficient priority-based experience storage and sampling
- **Multi-Platform Compatibility**: Automatic detection and utilization of CPU, CUDA (NVIDIA GPU), or MPS (Apple Silicon)
- **Detailed Visualization**: Provides visualizations of training progress, rewards, losses, priority distributions, and TD errors
- **Educational Implementation**: Includes detailed bilingual (English/Chinese) comments and algorithm explanations suitable for learning and research
- **Efficient Training**: Optimized environment preprocessing, experience sampling, and model architecture
- **Graceful Interruption Handling**: Training can be safely interrupted (Ctrl+C) with automatic checkpoint saving and resuming capability

*ä¸»è¦ç‰¹é»ï¼šå®Œæ•´çš„ PER å¯¦ç¾ã€å¤šå¹³å°å…¼å®¹ã€è©³ç´°è¦–è¦ºåŒ–ã€æ•™è‚²æ€§å¯¦ç¾ã€é«˜æ•ˆè¨“ç·´èˆ‡å„ªé›…çš„ä¸­æ–·è™•ç†*

## ğŸ› ï¸ Installation & Setup (å®‰è£èˆ‡è¨­ç½®)

### Prerequisites (å‰ææ¢ä»¶)

- Python 3.8+
- PyTorch 2.0+
- Gymnasium (newer version of OpenAI Gym)
- NumPy, Matplotlib, OpenCV
- Other dependencies listed in requirements.txt

### Installation Steps (å®‰è£æ­¥é©Ÿ)

1. Clone the repository
```bash
git clone https://github.com/yourusername/atari-ice-hockey-per.git
cd atari-ice-hockey-per
```

2. Install required dependencies
```bash
pip install -r requirements.txt
```

## ğŸ“Š Project Structure (å°ˆæ¡ˆçµæ§‹)

```
.
â”œâ”€â”€ config.py                    # Configuration file with all hyperparameters
â”œâ”€â”€ train.py                     # Training script to start the training process
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dqn_agent.py             # DQN agent implementation
â”‚   â”œâ”€â”€ per_memory.py            # Prioritized Experience Replay memory
â”‚   â”œâ”€â”€ sumtree.py               # SumTree data structure implementation
â”‚   â”œâ”€â”€ q_network.py             # Q-Network neural network architecture
â”‚   â”œâ”€â”€ env_wrappers.py          # Atari environment wrappers
â”‚   â”œâ”€â”€ device_utils.py          # Device detection and optimization utilities
â”‚   â”œâ”€â”€ logger.py                # Training logging tools
â”‚   â””â”€â”€ visualization.py         # Training metrics visualization tools
â”œâ”€â”€ results/                      # Directory for storing training results
â”‚   â”œâ”€â”€ data/                    # Training data
â”‚   â”œâ”€â”€ logs/                    # Training logs
â”‚   â”œâ”€â”€ models/                  # Saved models
â”‚   â””â”€â”€ plots/                   # Generated plots
â””â”€â”€ CHECKLIST.md                 # Implementation plan and pseudocode
```

## ğŸš€ Usage (ä½¿ç”¨æ–¹æ³•)

### Training a Model (è¨“ç·´æ¨¡å‹)

To train a new model from scratch:

```bash
python train.py
```


### Algorithm Description (æ¼”ç®—æ³•èªªæ˜)

This project implements the DQN algorithm with Prioritized Experience Replay:

1. **Prioritized Experience Replay (PER)**:
   - Efficiently stores and samples experiences using a SumTree data structure
   - Transitions are assigned priorities based on TD-error: p = (|Î´|+Îµ)^Î±
   - Importance sampling weights w = (NÂ·P(i))^(-Î²) correct the bias introduced
   - Î² gradually increases from 0.4 to 1.0 over time

2. **DQN Architecture**:
   - 3-layer convolutional neural network followed by fully connected layers
   - Dual network architecture (policy and target networks) with PER integration
   - Îµ-greedy exploration strategy with decaying Îµ value
   - Target network updates every 20,000 steps

## ğŸ“ˆ Experimental Results (å¯¦é©—çµæœ)

The training process generates various visualizations stored in the `results/plots` directory:

- Reward curves: Shows average rewards obtained per episode
- Loss curves: Shows how the network training loss changes
- Priority distribution: Shows the distribution of priorities in the experience replay
- Exploration rate curve: Shows how Îµ value changes over time

## ğŸ” Algorithm Details (ç®—æ³•ç´°ç¯€)

### SumTree Data Structure

The implemented SumTree has the following main operations:
- `add`: Add new experience with its priority
- `update_priority`: Update the priority of an experience
- `get_experience_by_priority`: Retrieve experience based on priority value

### Prioritized Experience Replay Mechanism

The PER memory implements:
- Priority calculation based on TD-errors
- Computation and proper application of importance sampling weights
- Linear annealing strategy for Î² value
- Efficient batch sampling

## ğŸ¤ Contributing (è²¢ç»)

Contributions and improvements are welcome! If you're interested in improving this project, please follow these steps:

1. Fork the repository
2. Create a new branch for your feature (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please ensure your code follows the project's coding style and includes appropriate tests.

è²¢ç»å’Œæ”¹é€²æ˜¯å—æ­¡è¿çš„ï¼å¦‚æœæ‚¨å°æ”¹é€²æ­¤å°ˆæ¡ˆæ„Ÿèˆˆè¶£ï¼Œè«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿæ“ä½œï¼š

1. Fork æ­¤å­˜å„²åº«
2. ç‚ºæ‚¨çš„åŠŸèƒ½å‰µå»ºä¸€å€‹æ–°åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'æ·»åŠ ä¸€äº›ä»¤äººé©šå˜†çš„åŠŸèƒ½'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. æ‰“é–‹ä¸€å€‹æ‹‰å–è«‹æ±‚

è«‹ç¢ºä¿æ‚¨çš„ä»£ç¢¼éµå¾ªé …ç›®çš„ç·¨ç¢¼é¢¨æ ¼ä¸¦åŒ…å«é©ç•¶çš„æ¸¬è©¦ã€‚

## ğŸ“„ License (è¨±å¯è­‰)

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT è¨±å¯è­‰ - è©³æƒ…è«‹åƒé–± [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ‘¨â€ğŸ’» Author (ä½œè€…)

Yitzu Liu - [@yitzuliu](https://github.com/yitzuliu)

## ğŸ™ Acknowledgements (è‡´è¬)

- This project is based on the [DQN paper](https://www.nature.com/articles/nature14236) by DeepMind and the [PER paper](https://arxiv.org/abs/1511.05952) by Google DeepMind
- Uses Atari game environments provided by [OpenAI Gymnasium](https://gymnasium.farama.org/)
- Special thanks to all researchers and developers in the reinforcement learning community

## ğŸ“Š Project Status (å°ˆæ¡ˆç‹€æ…‹)

This project is currently in active development. Core features are implemented and working, but optimizations and additional features are planned.

ç›®å‰è©²å°ˆæ¡ˆæ­£åœ¨ç©æ¥µé–‹ç™¼ä¸­ã€‚æ ¸å¿ƒåŠŸèƒ½å·²ç¶“å¯¦ç¾ä¸¦æ­£å¸¸é‹ä½œï¼Œä½†è¨ˆåŠƒé€²è¡Œå„ªåŒ–å’Œæ·»åŠ å…¶ä»–åŠŸèƒ½ã€‚