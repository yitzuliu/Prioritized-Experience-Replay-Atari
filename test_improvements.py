#!/usr/bin/env python3
"""
Test script for reliability and efficiency improvements.

This script tests the key improvements made to the PER DQN project
to ensure they are working correctly.

æ¸¬è©¦è…³æœ¬ï¼Œç”¨æ–¼é©—è­‰å¯é æ€§å’Œæ•ˆç‡æ”¹é€²ã€‚

æ­¤è…³æœ¬æ¸¬è©¦å° PER DQN é …ç›®æ‰€åšçš„é—œéµæ”¹é€²ï¼Œä»¥ç¢ºä¿å®ƒå€‘æ­£å¸¸å·¥ä½œã€‚
"""

import sys
import os
import time
import traceback
import numpy as np

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_configuration_validation():
    """Test configuration validation system."""
    print("ğŸ§ª Testing Configuration Validation...")
    
    try:
        import config
        
        # Test configuration summary
        if hasattr(config, 'get_config_summary'):
            summary = config.get_config_summary()
            assert 'environment' in summary
            assert 'training' in summary
            assert 'per' in summary
            print("âœ… Configuration summary working")
        else:
            print("âš ï¸ Configuration summary not available")
        
        # Test validation
        if hasattr(config, 'validate_config'):
            errors = config.validate_config()
            if not errors:
                print("âœ… Configuration validation passed")
            else:
                print(f"âš ï¸ Configuration validation found issues: {errors}")
        else:
            print("âš ï¸ Configuration validation not available")
            
        return True
        
    except Exception as e:
        print(f"âŒ Configuration validation test failed: {str(e)}")
        return False

def test_enhanced_per_memory():
    """Test enhanced PER memory implementation."""
    print("\nğŸ§ª Testing Enhanced PER Memory...")
    
    try:
        from src.per_memory import PERMemory
        
        # Create PER memory instance
        memory = PERMemory(memory_capacity=100)
        
        # Test memory usage monitoring
        if hasattr(memory, 'get_memory_usage'):
            usage = memory.get_memory_usage()
            assert 'rss_bytes' in usage
            assert 'percent' in usage
            print("âœ… Memory usage monitoring working")
        else:
            print("âš ï¸ Memory usage monitoring not available")
        
        # Test performance stats
        if hasattr(memory, 'get_performance_stats'):
            stats = memory.get_performance_stats()
            assert 'cache_hit_rate' in stats
            assert 'memory_usage' in stats
            print("âœ… Performance statistics working")
        else:
            print("âš ï¸ Performance statistics not available")
        
        # Test adding transitions with validation
        try:
            # Valid transition
            state = np.random.rand(4, 84, 84)
            memory.add(state, 1, 1.0, state, False)
            
            # Test invalid transitions (should be caught)
            try:
                memory.add(None, 1, 1.0, state, False)  # Invalid state
                print("âš ï¸ Invalid state not caught")
            except ValueError:
                print("âœ… Input validation working")
            
        except Exception as e:
            print(f"âš ï¸ Transition validation test failed: {str(e)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Enhanced PER memory test failed: {str(e)}")
        return False

def test_performance_monitoring():
    """Test performance monitoring system."""
    print("\nğŸ§ª Testing Performance Monitoring...")
    
    try:
        from src.performance_monitor import PerformanceMonitor, HyperparameterTuner
        
        # Test performance monitor
        monitor = PerformanceMonitor(monitor_interval=0.1, history_size=10)
        
        # Test timing context manager
        if hasattr(monitor, 'time_operation'):
            with monitor.time_operation('test_operation'):
                time.sleep(0.01)  # Simulate some work
            print("âœ… Timing context manager working")
        else:
            print("âš ï¸ Timing context manager not available")
        
        # Test monitoring start/stop
        if hasattr(monitor, 'start_monitoring') and hasattr(monitor, 'stop_monitoring'):
            monitor.start_monitoring()
            time.sleep(0.2)  # Let it collect some data
            monitor.stop_monitoring()
            print("âœ… Background monitoring working")
        else:
            print("âš ï¸ Background monitoring not available")
        
        # Test performance reporting
        if hasattr(monitor, 'get_performance_report'):
            report = monitor.get_performance_report()
            assert 'timestamp' in report
            assert 'system_performance' in report
            print("âœ… Performance reporting working")
        else:
            print("âš ï¸ Performance reporting not available")
        
        # Test hyperparameter tuner
        tuner = HyperparameterTuner(monitor)
        if hasattr(tuner, 'get_tuning_recommendations'):
            metrics = {
                'loss_trend': 0.1,
                'loss_variance': 0.05,
                'reward_trend': 1.0,
                'avg_fps': 50,
                'memory_usage': 0.7
            }
            recommendations = tuner.get_tuning_recommendations(metrics)
            assert 'timestamp' in recommendations
            print("âœ… Hyperparameter tuning working")
        else:
            print("âš ï¸ Hyperparameter tuning not available")
        
        return True
        
    except ImportError:
        print("âš ï¸ Performance monitoring module not available")
        return True  # Not a failure if module is not available
    except Exception as e:
        print(f"âŒ Performance monitoring test failed: {str(e)}")
        return False

def test_enhanced_agent():
    """Test enhanced DQN agent capabilities."""
    print("\nğŸ§ª Testing Enhanced DQN Agent...")
    
    try:
        from src.dqn_agent import DQNAgent
        import torch
        import tempfile
        
        # Create agent
        agent = DQNAgent(
            state_shape=(4, 84, 84),
            action_space_size=9,
            learning_rate=0.0001,
            use_per=True
        )
        
        # Test training diagnostics
        if hasattr(agent, 'get_training_diagnostics'):
            diagnostics = agent.get_training_diagnostics()
            assert 'training_progress' in diagnostics
            assert 'network_info' in diagnostics
            print("âœ… Training diagnostics working")
        else:
            print("âš ï¸ Training diagnostics not available")
        
        # Test enhanced model saving/loading
        with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as f:
            temp_path = f.name
        
        try:
            # Test saving with metadata
            metadata = {'test': 'enhanced_save'}
            success = agent.save_model(temp_path, metadata=metadata)
            
            if success:
                print("âœ… Enhanced model saving working")
                
                # Test enhanced loading
                if hasattr(agent, 'load_model'):
                    load_result = agent.load_model(temp_path)
                    if isinstance(load_result, dict) and load_result.get('success', False):
                        print("âœ… Enhanced model loading working")
                    else:
                        print("âš ï¸ Enhanced model loading returned unexpected result")
                else:
                    print("âš ï¸ Enhanced model loading not available")
            else:
                print("âš ï¸ Enhanced model saving failed")
        
        finally:
            # Cleanup
            if os.path.exists(temp_path):
                os.unlink(temp_path)
        
        return True
        
    except Exception as e:
        print(f"âŒ Enhanced DQN agent test failed: {str(e)}")
        return False

def test_resource_monitoring():
    """Test resource monitoring capabilities."""
    print("\nğŸ§ª Testing Resource Monitoring...")
    
    try:
        import psutil
        
        # Test basic resource monitoring
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_percent = process.memory_percent()
        
        assert memory_info.rss > 0
        assert 0 <= memory_percent <= 100
        print("âœ… Basic resource monitoring working")
        
        # Test enhanced resource checking (if available in train.py)
        try:
            from train import enhanced_resource_check
            status = enhanced_resource_check()
            assert 'memory_percent' in status
            assert 'warnings' in status
            print("âœ… Enhanced resource checking working")
        except ImportError:
            print("âš ï¸ Enhanced resource checking not available")
        
        return True
        
    except Exception as e:
        print(f"âŒ Resource monitoring test failed: {str(e)}")
        return False

def main():
    """Run all tests for reliability and efficiency improvements."""
    print("ğŸš€ Testing Reliability and Efficiency Improvements")
    print("=" * 60)
    
    tests = [
        ("Configuration Validation", test_configuration_validation),
        ("Enhanced PER Memory", test_enhanced_per_memory),
        ("Performance Monitoring", test_performance_monitoring),
        ("Enhanced DQN Agent", test_enhanced_agent),
        ("Resource Monitoring", test_resource_monitoring),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = result
        except Exception as e:
            print(f"âŒ {test_name} test crashed: {str(e)}")
            print(f"Traceback: {traceback.format_exc()}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ Test Results Summary:")
    
    passed = 0
    total = 0
    
    for test_name, result in results.items():
        total += 1
        if result:
            passed += 1
            print(f"âœ… {test_name}: PASSED")
        else:
            print(f"âŒ {test_name}: FAILED")
    
    print(f"\nğŸ¯ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ All reliability and efficiency improvements are working correctly!")
        return 0
    else:
        print("âš ï¸ Some improvements may need attention.")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 